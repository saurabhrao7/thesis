{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "# Reading the train data and storing them in a single dataframe\n",
    "data_files = ['Youtube01-Psy.csv','Youtube02-KatyPerry.csv','Youtube03-LMFAO.csv','Youtube04-Eminem.csv','Youtube05-Shakira.csv']\n",
    "for file in data_files:\n",
    "    data = pd.read_csv(file)\n",
    "    train_data.append(data)\n",
    "train_data = pd.concat(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function which drops the given features from the given dataframe\n",
    "def drop_features(features,data):\n",
    "    data.drop(features,axis=1,inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data\n",
    "- Drop insignificant columns\n",
    "- Process the contents of data\n",
    "- Extract features from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop insignificant columns\n",
    "drop_features(['COMMENT_ID','AUTHOR','DATE'],train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1956 entries, 0 to 369\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   CONTENT  1956 non-null   object\n",
      " 1   CLASS    1956 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 45.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Huh, anyway check out this you[tube] channel: kobyoshi02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey guys check out my new channel and our first vid THIS IS US THE  MONKEYS!!! I'm the monkey in the white shirt,please leave a like comment  and please subscribe!!!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                  CONTENT  \\\n",
       "0                                                                                                                Huh, anyway check out this you[tube] channel: kobyoshi02   \n",
       "1  Hey guys check out my new channel and our first vid THIS IS US THE  MONKEYS!!! I'm the monkey in the white shirt,please leave a like comment  and please subscribe!!!!   \n",
       "2                                                                                                                                  just for test I have to say murdev.com   \n",
       "3                                                                                                                        me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4                                                                                                                                 watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "\n",
       "   CLASS  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1956 entries, 0 to 369\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   CONTENT  1956 non-null   object\n",
      " 1   CLASS    1956 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 45.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all words\n",
    "train_data['processed_content'] = train_data['CONTENT'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features(['CONTENT'],train_data)\n",
    "train_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1005\n",
       "0     951\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['CLASS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS</th>\n",
       "      <th>processed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>huh anyway check out this you tube channel kobyoshi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hey guys check out my new channel and our first vid this is us the monkeys i m the monkey in the white shirt please leave a like comment and please subscribe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>just for test i have to say murdev com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>watch v vtarggvgtwq check this out</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CLASS  \\\n",
       "0      1   \n",
       "1      1   \n",
       "2      1   \n",
       "3      1   \n",
       "4      1   \n",
       "\n",
       "                                                                                                                                               processed_content  \n",
       "0                                                                                                            huh anyway check out this you tube channel kobyoshi  \n",
       "1  hey guys check out my new channel and our first vid this is us the monkeys i m the monkey in the white shirt please leave a like comment and please subscribe  \n",
       "2                                                                                                                         just for test i have to say murdev com  \n",
       "3                                                                                                                     me shaking my sexy ass on my channel enjoy  \n",
       "4                                                                                                                             watch v vtarggvgtwq check this out  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "train_data['processed_content'] = train_data['processed_content'].apply(lambda x: \" \".join(re.findall(\"[A-Za-z]+\", x.lower())))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with blank values\n",
    "train_data = train_data[~train_data['processed_content'].str.strip().eq('')]\n",
    "train_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of comments is 17.\n",
      "Max word length of comments is 218.\n",
      "Min word length of comments is 1.\n"
     ]
    }
   ],
   "source": [
    "# tokenize all comments in the data\n",
    "comments_token = train_data['processed_content'].apply(lambda x: x.split())\n",
    "print('Average word length of comments is {0:.0f}.'.format(np.mean(comments_token.apply(lambda x: len(x)))))\n",
    "print('Max word length of comments is {0:.0f}.'.format(np.max(comments_token.apply(lambda x: len(x)))))\n",
    "print('Min word length of comments is {0:.0f}.'.format(np.min(comments_token.apply(lambda x: len(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Words Removal\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "comments_rsw = []\n",
    "for comment_token in comments_token:\n",
    "    filtered_comment = [w for w in comment_token if not w in stop_words] \n",
    "    comments_rsw.append(filtered_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "def lemmatization(dataset):\n",
    "    lemma = nltk.WordNetLemmatizer()\n",
    "    comments_lemma = []\n",
    "    for comment in comments_rsw:\n",
    "        comment_lemma = [lemma.lemmatize(w) for w in comment]\n",
    "        comments_lemma.append(comment_lemma)\n",
    "    return comments_lemma\n",
    "\n",
    "# Words lemmatization\n",
    "comments_lemma = lemmatization(comments_rsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "labels = train_data['CLASS'].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(comments_lemma, \n",
    "                                                    labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Create bag of words\n",
    "X_train_untokenized = [' '.join(comment) for comment in X_train]\n",
    "X_test_untokenized = [' '.join(comment) for comment in X_test]\n",
    "\n",
    "# Use HashingVectorizer\n",
    "vect = HashingVectorizer(n_features=10000)  # Set the desired number of features\n",
    "x_train_hash = vect.transform(X_train_untokenized)\n",
    "x_test_hash = vect.transform(X_test_untokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'C': [i / 1000 for i in range(1, 1001)],\n",
    "    'loss': ['hinge', 'squared_hinge']\n",
    "}\n",
    "\n",
    "# Create an instance of PassiveAggressiveClassifier\n",
    "clf = PassiveAggressiveClassifier()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "cv_split = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=cv_split)\n",
    "grid_search.fit(x_train_hash, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding score\n",
    "print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "\n",
    "# Create an instance of PassiveAggressiveClassifier\n",
    "clf = PassiveAggressiveClassifier(C=0.01, loss='squared_hinge')\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "clf.fit(x_train_hash, y_train)\n",
    "\n",
    "# Predict labels for the train data\n",
    "y_train_pred = clf.predict(x_train_hash)\n",
    "\n",
    "# Predict labels for the test data\n",
    "y_pred = clf.predict(x_test_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[748,   5],\n",
       "       [  5, 798]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train,y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       753\n",
      "           1       0.99      0.99      0.99       803\n",
      "\n",
      "    accuracy                           0.99      1556\n",
      "   macro avg       0.99      0.99      0.99      1556\n",
      "weighted avg       0.99      0.99      0.99      1556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_report = classification_report(y_train,y_train_pred)\n",
    "print(\"Train Classification Report:\\n\", train_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[181,   7],\n",
       "       [  9, 192]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96       188\n",
      "           1       0.96      0.96      0.96       201\n",
      "\n",
      "    accuracy                           0.96       389\n",
      "   macro avg       0.96      0.96      0.96       389\n",
      "weighted avg       0.96      0.96      0.96       389\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_report = classification_report(y_test,y_pred)\n",
    "print(\"Test Classification Report:\\n\", test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9588688946015425\n",
      "Precision = 0.964824120603015\n",
      "Recall = 0.9552238805970149\n",
      "F1-Score = 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "# Extract the TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]  # true positive \n",
    "TN = cm[0, 0]  # true negative\n",
    "FP = cm[0, 1]  # false positive\n",
    "FN = cm[1, 0]  # false negative\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = TP / float(TP + FP)\n",
    "recall = TP / float(TP + FN)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy =\", (TP + TN) / float(TP + TN + FP + FN))\n",
    "print(\"Precision =\", TP / float(TP + FP))\n",
    "print(\"Recall =\", TP / float(TP + FN))\n",
    "print(\"F1-Score =\", 2 * (precision * recall) / (precision + recall))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91       493\n",
      "           1       0.90      0.92      0.91       499\n",
      "\n",
      "    accuracy                           0.91       992\n",
      "   macro avg       0.91      0.91      0.91       992\n",
      "weighted avg       0.91      0.91      0.91       992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "addl_df = pd.read_csv('5000 YT comments.csv', encoding='Windows-1252')\n",
    "# drop insignificant columns\n",
    "drop_features(['Name', 'Time', 'Likes', 'Reply Count'], addl_df)\n",
    "addl_df = addl_df.rename(columns={'Comment': 'CONTENT', 'Spam': 'CLASS'})\n",
    "\n",
    "# Preprocess the additional data\n",
    "addl_df['processed_content'] = addl_df['CONTENT'].apply(lambda x: x.lower())\n",
    "addl_df['processed_content'] = addl_df['processed_content'].apply(lambda x: \" \".join(re.findall(\"[A-Za-z]+\", x.lower())))\n",
    "addl_df = addl_df[~addl_df['processed_content'].str.strip().eq('')]\n",
    "addl_df.reset_index(drop=True, inplace=True)\n",
    "addl_df['processed_content'] = addl_df['processed_content'].apply(lambda x: [WordNetLemmatizer().lemmatize(w) for w in x.split() if w not in stop_words])\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df, test_df, train_labels, test_labels = train_test_split(addl_df['processed_content'], addl_df['CLASS'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Transform the data using the existing vectorizer\n",
    "x_train_hash = vect.transform([' '.join(comment) for comment in train_df]).toarray()\n",
    "y_train = train_labels.values\n",
    "\n",
    "x_test_hash = vect.transform([' '.join(comment) for comment in test_df]).toarray()\n",
    "y_test = test_labels.values\n",
    "\n",
    "# Update the model with additional data\n",
    "x_addl_hash = vect.transform([' '.join(comment) for comment in train_df]).toarray()\n",
    "y_addl = train_labels.values\n",
    "\n",
    "clf.set_params(C=0.1, loss= 'squared_hinge')\n",
    "clf.partial_fit(x_addl_hash, y_addl)\n",
    "\n",
    "# Perform prediction on the updated model\n",
    "y_train_pred = clf.predict(x_train_hash)\n",
    "y_pred = clf.predict(x_test_hash)\n",
    "y_addl_pred = clf.predict(x_addl_hash)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.907258064516129\n",
      "Precision = 0.9013806706114399\n",
      "Recall = 0.9158316633266533\n",
      "F1-Score = 0.9085487077534792\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "# Extract the TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]  # true positive \n",
    "TN = cm[0, 0]  # true negative\n",
    "FP = cm[0, 1]  # false positive\n",
    "FN = cm[1, 0]  # false negative\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = TP / float(TP + FP)\n",
    "recall = TP / float(TP + FN)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy =\", (TP + TN) / float(TP + TN + FP + FN))\n",
    "print(\"Precision =\", TP / float(TP + FP))\n",
    "print(\"Recall =\", TP / float(TP + FN))\n",
    "print(\"F1-Score =\", 2 * (precision * recall) / (precision + recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2b4e89fcae5b91aa8ea29bb1c9dcc5e94352bc4a29da12e91d68cb992a2a532"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
